{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "Before working on this assignment please read these instructions fully. In the submission area, you will notice that you can click the link to **Preview the Grading** for each step of the assignment. This is the criteria that will be used for peer grading. Please familiarize yourself with the criteria before beginning the assignment.\n",
    "\n",
    "An NOAA dataset has been stored in the file `data/C2A2_data/BinnedCsvs_d400/fb441e62df2d58994928907a91895ec62c2c42e6cd075c2700843b89.csv`. The data for this assignment comes from a subset of The National Centers for Environmental Information (NCEI) [Daily Global Historical Climatology Network](https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt) (GHCN-Daily). The GHCN-Daily is comprised of daily climate records from thousands of land surface stations across the globe.\n",
    "\n",
    "Each row in the assignment datafile corresponds to a single observation.\n",
    "\n",
    "The following variables are provided to you:\n",
    "\n",
    "* **id** : station identification code\n",
    "* **date** : date in YYYY-MM-DD format (e.g. 2012-01-24 = January 24, 2012)\n",
    "* **element** : indicator of element type\n",
    "    * TMAX : Maximum temperature (tenths of degrees C)\n",
    "    * TMIN : Minimum temperature (tenths of degrees C)\n",
    "* **value** : data value for element (tenths of degrees C)\n",
    "\n",
    "For this assignment, you must:\n",
    "\n",
    "1. Read the documentation and familiarize yourself with the dataset, then write some python code which returns a line graph of the record high and record low temperatures by day of the year over the period 2005-2014. The area between the record high and record low temperatures for each day should be shaded.\n",
    "2. Overlay a scatter of the 2015 data for any points (highs and lows) for which the ten year record (2005-2014) record high or record low was broken in 2015.\n",
    "3. Watch out for leap days (i.e. February 29th), it is reasonable to remove these points from the dataset for the purpose of this visualization.\n",
    "4. Make the visual nice! Leverage principles from the first module in this course when developing your solution. Consider issues such as legends, labels, and chart junk.\n",
    "\n",
    "The data you have been given is near **Ann Arbor, Michigan, United States**, and the stations the data comes from are shown on the map below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'data/C2A2_data/BinSize_d400.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-aad554fcc6ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmplleaflet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mleaflet_plot_stations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'fb441e62df2d58994928907a91895ec62c2c42e6cd075c2700843b89'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-aad554fcc6ec>\u001b[0m in \u001b[0;36mleaflet_plot_stations\u001b[0;34m(binsize, hashid)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mleaflet_plot_stations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhashid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/C2A2_data/BinSize_d{}.csv'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mstation_locations_by_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hash'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mhashid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'data/C2A2_data/BinSize_d400.csv' does not exist"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mplleaflet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def leaflet_plot_stations(binsize, hashid):\n",
    "\n",
    "    df = pd.read_csv('data/C2A2_data/BinSize_d{}.csv'.format(binsize))\n",
    "\n",
    "    station_locations_by_hash = df[df['hash'] == hashid]\n",
    "\n",
    "    lons = station_locations_by_hash['LONGITUDE'].tolist()\n",
    "    lats = station_locations_by_hash['LATITUDE'].tolist()\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "\n",
    "    plt.scatter(lons, lats, c='r', alpha=0.7, s=200)\n",
    "\n",
    "    return mplleaflet.display()\n",
    "\n",
    "leaflet_plot_stations(400,'fb441e62df2d58994928907a91895ec62c2c42e6cd075c2700843b89')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.ticker import NullFormatter\n",
    "from matplotlib.dates import MonthLocator, DateFormatter\n",
    "\n",
    "df = pd.read_csv('data/C2A2_data/BinnedCsvs_d400/fb441e62df2d58994928907a91895ec62c2c42e6cd075c2700843b89.csv')\n",
    "df.set_index(\"ID\", inplace=True)\n",
    "df_UMich = df.loc['USC00200230'].sort(['Date'])\n",
    "df_UMich = df_UMich.reset_index()\n",
    "\n",
    "# Here, let's use only data from 2005 to 2014, not 2015, and re-name the dataframe as df_UMich\n",
    "df_UMich_2005_2014 = df_UMich.loc[df_UMich['Date'].apply(lambda x: x.split('-')[0] in ['2005','2006','2007','2008','2009','2010','2011','2012','2013','2014'] )] \n",
    "\n",
    "df_UMich_TMAX_2005_2014 = df_UMich_2005_2014.loc[df_UMich['Element'] == 'TMAX']\n",
    "df_UMich_TMIN_2005_2014 = df_UMich_2005_2014.loc[df_UMich['Element'] == 'TMIN']\n",
    "\n",
    "# list of all days without year\n",
    "days_list = []\n",
    "for date in df_UMich_2005_2014['Date']:\n",
    "    if '2008' in date:\n",
    "        days_list.append(date)\n",
    "\n",
    "days_without_year_list = []\n",
    "for date in days_list:\n",
    "    days_without_year_list.append(date[5:])\n",
    "\n",
    "# Next, get record high and record low of each date between 2005 to 2014\n",
    "date_TMAX_dic_2005_2014 = {}\n",
    "    \n",
    "date_TMAX_dic_2005_2014 = {day_el: [] for day_el in days_without_year_list}\n",
    "\n",
    "for day_el in days_without_year_list:\n",
    "    for el_df_UMich_TMAX_2005_2014 in df_UMich_TMAX_2005_2014['Date']:\n",
    "        if day_el in el_df_UMich_TMAX_2005_2014:\n",
    "            date_TMAX_dic_2005_2014[day_el].append(df_UMich_TMAX_2005_2014.loc[df_UMich_TMAX_2005_2014['Date'] == el_df_UMich_TMAX_2005_2014]['Data_Value'])\n",
    "            \n",
    "\n",
    "df_dic = date_TMAX_dic_2005_2014\n",
    "date_TMAX_dic_2005_2014_2 = {}\n",
    "date_TMAX_dic_2005_2014_2 = {day_el: [-500] for day_el in days_without_year_list}\n",
    "\n",
    "for day_dic_el in df_dic:\n",
    "    for el in df_dic[day_dic_el]:\n",
    "        if el.values[0] > date_TMAX_dic_2005_2014_2[day_dic_el]:\n",
    "            date_TMAX_dic_2005_2014_2[day_dic_el] = el.values[0]\n",
    "\n",
    "date_TMAX_dic_2005_2014_2.pop('02-29')\n",
    "\n",
    "#Now I got max temp and min temp on each day\n",
    "date_TMIN_dic_2005_2014 = {}\n",
    "    \n",
    "date_TMIN_dic_2005_2014 = {day_el: [] for day_el in days_without_year_list}\n",
    "\n",
    "for day_el in days_without_year_list:\n",
    "    for el_df_UMich_TMIN_2005_2014 in df_UMich_TMIN_2005_2014['Date']:\n",
    "        if day_el in el_df_UMich_TMIN_2005_2014:\n",
    "            date_TMIN_dic_2005_2014[day_el].append(df_UMich_TMIN_2005_2014.loc[df_UMich_TMIN_2005_2014['Date'] == el_df_UMich_TMIN_2005_2014]['Data_Value'])\n",
    "            \n",
    "df_dic = date_TMIN_dic_2005_2014\n",
    "date_TMIN_dic_2005_2014_2 = {}\n",
    "date_TMIN_dic_2005_2014_2 = {day_el: [500] for day_el in days_without_year_list}\n",
    "\n",
    "for day_dic_el in df_dic:\n",
    "    for el in df_dic[day_dic_el]:\n",
    "        if el.values[0] < date_TMIN_dic_2005_2014_2[day_dic_el]:\n",
    "            date_TMIN_dic_2005_2014_2[day_dic_el] = el.values[0]\n",
    "\n",
    "date_TMIN_dic_2005_2014_2.pop('02-29')\n",
    "\n",
    "# do the same thing with only 2015 data\n",
    "\n",
    "df_UMich_2015 = df_UMich.loc[df_UMich['Date'].apply(lambda x: x.split('-')[0] == '2015')]\n",
    "df_UMich_TMAX_2015 = df_UMich_2015.loc[df_UMich_2015['Element'] == 'TMAX']\n",
    "df_UMich_TMIN_2015 = df_UMich_2015.loc[df_UMich_2015['Element'] == 'TMIN']\n",
    "\n",
    "date_TMAX_dic_2015 = {}\n",
    "    \n",
    "date_TMAX_dic_2015 = {day_el: [] for day_el in days_without_year_list}\n",
    "\n",
    "for day_el in days_without_year_list:\n",
    "    for el_df_UMich_TMAX_2015 in df_UMich_TMAX_2015['Date']:\n",
    "        if day_el in el_df_UMich_TMAX_2015:\n",
    "            date_TMAX_dic_2015[day_el].append(df_UMich_TMAX_2015.loc[df_UMich_TMAX_2015['Date'] == el_df_UMich_TMAX_2015]['Data_Value'])\n",
    "            \n",
    "df_dic = date_TMAX_dic_2015\n",
    "date_TMAX_dic_2015_2 = {}\n",
    "date_TMAX_dic_2015_2 = {day_el: [-500] for day_el in days_without_year_list}\n",
    "\n",
    "for day_dic_el in df_dic:\n",
    "    for el in df_dic[day_dic_el]:\n",
    "        if el.values[0] > date_TMAX_dic_2015_2[day_dic_el]:\n",
    "            date_TMAX_dic_2015_2[day_dic_el] = el.values[0]\n",
    "\n",
    "date_TMAX_dic_2015_2.pop('02-29')\n",
    "\n",
    "date_TMIN_dic_2015 = {}\n",
    "    \n",
    "date_TMIN_dic_2015 = {day_el: [] for day_el in days_without_year_list}\n",
    "\n",
    "for day_el in days_without_year_list:\n",
    "    for el_df_UMich_TMIN_2015 in df_UMich_TMIN_2015['Date']:\n",
    "        if day_el in el_df_UMich_TMIN_2015:\n",
    "            date_TMIN_dic_2015[day_el].append(df_UMich_TMIN_2015.loc[df_UMich_TMIN_2015['Date'] == el_df_UMich_TMIN_2015]['Data_Value'])\n",
    "            \n",
    "\n",
    "df_dic = date_TMIN_dic_2015\n",
    "date_TMIN_dic_2015_2 = {}\n",
    "date_TMIN_dic_2015_2 = {day_el: [-500] for day_el in days_without_year_list}\n",
    "\n",
    "for day_dic_el in df_dic:\n",
    "    for el in df_dic[day_dic_el]:\n",
    "        if el.values[0] > date_TMIN_dic_2015_2[day_dic_el]:\n",
    "            date_TMIN_dic_2015_2[day_dic_el] = el.values[0]\n",
    "\n",
    "date_TMIN_dic_2015_2.pop('02-29')\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "one_year_array = pd.Series(range(1,len(date_TMAX_dic_2005_2014_2)+1)).values\n",
    "\n",
    "\n",
    "TMAX_df_from_dic_to_array = pd.DataFrame.from_dict(date_TMAX_dic_2005_2014_2, orient='index').values\n",
    "TMIN_df_from_dic_to_array = pd.DataFrame.from_dict(date_TMIN_dic_2005_2014_2, orient='index').values\n",
    "\n",
    "# convert an array to a list and divided by ten, then convert back to an array from the list\n",
    "TMAX_df_from_dic_to_array_to_list_of_list = TMAX_df_from_dic_to_array.tolist()\n",
    "TMAX_df_from_dic_to_array_to_list = []\n",
    "for el in TMAX_df_from_dic_to_array_to_list_of_list:\n",
    "    TMAX_df_from_dic_to_array_to_list.append(el[0])\n",
    "TMAX_df_from_dic_to_array_from_list = np.array([x/10 for x in TMAX_df_from_dic_to_array_to_list])\n",
    "\n",
    "TMIN_df_from_dic_to_array_to_list_of_list = TMIN_df_from_dic_to_array.tolist()\n",
    "TMIN_df_from_dic_to_array_to_list = []\n",
    "for el in TMIN_df_from_dic_to_array_to_list_of_list:\n",
    "    TMIN_df_from_dic_to_array_to_list.append(el[0])\n",
    "TMIN_df_from_dic_to_array_from_list = np.array([x/10 for x in TMIN_df_from_dic_to_array_to_list])\n",
    "\n",
    "# Next, find dates when 2015 broke the record high and low bewteen 2005 and 2014\n",
    "count_1 = 1\n",
    "TMAX_broke_day_index_list = []\n",
    "TMAX_borke_temp_list = []\n",
    "for el_1,el_2 in date_TMAX_dic_2015_2.items():\n",
    "    if el_2 > date_TMAX_dic_2005_2014_2.get(el_1):\n",
    "        TMAX_broke_day_index_list.append(count_1)\n",
    "        TMAX_borke_temp_list.append(el_2)\n",
    "        count_1 += 1\n",
    "    else:\n",
    "        count_1 += 1\n",
    "\n",
    "count_2 = 1\n",
    "TMIN_broke_day_index_list = []\n",
    "TMIN_borke_temp_list = []\n",
    "for el_1,el_2 in date_TMIN_dic_2015_2.items():\n",
    "    if el_2 < date_TMIN_dic_2005_2014_2.get(el_1):\n",
    "        TMIN_broke_day_index_list.append(count_2)\n",
    "        TMIN_borke_temp_list.append(el_2)\n",
    "        count_2 += 1\n",
    "    else:\n",
    "        count_2 += 1\n",
    "\n",
    "TMAX_broke_day_array = np.array(TMAX_broke_day_index_list)\n",
    "TMIN_broke_day_array = np.array(TMIN_broke_day_index_list)\n",
    "TMAX_broke_temp_array = np.array([x/10 for x in TMAX_borke_temp_list])\n",
    "TMIN_broke_temp_array = np.array([x/10 for x in TMIN_borke_temp_list])\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "observation_dates = np.arange('2015-01-01', '2016-01-01', dtype = 'datetime64[D]')\n",
    "\n",
    "count_3 = 1\n",
    "TMAX_broke_obs_date_list = []\n",
    "for obs_dates_el in observation_dates.tolist():\n",
    "    if count_3 in TMAX_broke_day_index_list:\n",
    "        TMAX_broke_obs_date_list.append(obs_dates_el)\n",
    "        count_3 += 1\n",
    "    else:\n",
    "        count_3 += 1\n",
    "TMAX_broke_obs_date_array = np.array(TMAX_broke_obs_date_list)\n",
    "\n",
    "\n",
    "count_4 = 1\n",
    "TMIN_broke_obs_date_list = []\n",
    "for obs_dates_el in observation_dates.tolist():\n",
    "    if count_4 in TMIN_broke_day_index_list:\n",
    "        TMIN_broke_obs_date_list.append(obs_dates_el)\n",
    "        count_4 += 1\n",
    "    else:\n",
    "        count_4 += 1\n",
    "TMIN_broke_obs_date_array = np.array(TMIN_broke_obs_date_list)    \n",
    "    \n",
    "plt.plot(observation_dates, TMAX_df_from_dic_to_array_from_list, 'tab:orange', zorder = 1, \n",
    "         label = '10-year (2005-2014) Record High')\n",
    "plt.plot(observation_dates, TMIN_df_from_dic_to_array_from_list, 'xkcd:lightblue', zorder = 1,\n",
    "         label = '10-year (2005-2014) Record Low')\n",
    "plt.plot(TMAX_broke_obs_date_array, TMAX_broke_temp_array, 'ro', zorder = 1, \n",
    "         label = '2015 New High')\n",
    "plt.plot(TMIN_broke_obs_date_array, TMIN_broke_temp_array, 'bo', zorder = 1, \n",
    "         label = '2015 New Low')\n",
    "\n",
    "ax = plt.gca()\n",
    "# Set title\n",
    "from textwrap import wrap\n",
    "ax.set_title(\"\\n\".join(wrap('Record High and Low Temperatures near Ann Arbor by Day',35)))\n",
    "# Set labels for the y axes.\n",
    "ax.set_ylabel('$^\\circ C$', rotation = 0, position = (0, 0.94))\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "\n",
    "# Shade the area between the area between the record high and record low temperatures.\n",
    "ax.fill_between(x = observation_dates, \n",
    "                y1 = TMAX_df_from_dic_to_array_from_list, y2 = TMIN_df_from_dic_to_array_from_list, \n",
    "                facecolor = 'lavenderblush')\n",
    "\n",
    "# Add the 0 degree celsius line.\n",
    "plt.axhline(linestyle = '--', color = 'gray', linewidth = 0.3) \n",
    "\n",
    "################ Deal with the legend, axis, title, etc. ##################\n",
    "plt.legend(bbox_to_anchor = (1.8, 0.5), frameon = False)\n",
    "\n",
    "# Increase the left and right margin to better show the edge points.\n",
    "ax.set_xlim(observation_dates[0] - 10, observation_dates[364] + 10)\n",
    "\n",
    "# Set major x ticks at the beginning/end of each month and add minor ticks at the \n",
    "# 15th day of each month.\n",
    "ax.xaxis.set_major_locator(MonthLocator())\n",
    "ax.xaxis.set_minor_locator(MonthLocator(bymonthday = 15))\n",
    "\n",
    "# Put the abbr. month labels in the middle of of the span of each month.\n",
    "ax.xaxis.set_major_formatter(NullFormatter())\n",
    "ax.xaxis.set_minor_formatter(DateFormatter('%b'))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
